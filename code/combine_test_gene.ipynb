{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler, QuantileTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# (Optional) matplotlib\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    has_matplotlib = True\n",
    "except ImportError:\n",
    "    has_matplotlib = False\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) DEAP의 creator 중복 생성 방지\n",
    "# ------------------------------------------------\n",
    "def create_deap_classes():\n",
    "    try:\n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    except AttributeError:\n",
    "        pass  # 이미 생성되었으면 무시\n",
    "\n",
    "    try:\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "    except AttributeError:\n",
    "        pass  # 이미 생성되었으면 무시\n",
    "\n",
    "create_deap_classes()\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) 개체(Individual) 생성\n",
    "#    - 각 특징을 선택(1) / 제외(0)\n",
    "# ------------------------------------------------\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "\n",
    "def initialize_individual():\n",
    "    return tools.initRepeat(creator.Individual, toolbox.attr_bool, n=X.shape[1])\n",
    "\n",
    "toolbox.register(\"individual\", initialize_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) GA 평가함수: XGB + GridSearchCV\n",
    "# ------------------------------------------------\n",
    "def evalFeatureSelection(individual, X, y, cv_folds=4):\n",
    "    \"\"\"\n",
    "    individual: 0/1로 구성된 feature mask\n",
    "    X, y: 전체 데이터 (numpy array 등)\n",
    "    cv_folds: StratifiedKFold 폴드 수\n",
    "    \"\"\"\n",
    "    # 선택된 피처 인덱스 추출\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    # 아무 피처도 선택되지 않은 경우\n",
    "    if len(selected_features) == 0:\n",
    "        return (0.0,)\n",
    "\n",
    "    # XGBoost 하이퍼파라미터 그리드 (예시)\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # StratifiedKFold 정의\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # f1_macro를 목표로 GridSearch\n",
    "    scoring = make_scorer(f1_score, average='weighted')\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        cv=skf,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # 선택된 피처만으로 학습\n",
    "    grid_search.fit(X[:, selected_features], y)\n",
    "\n",
    "    # best_score_는 폴드 평균 f1_macro\n",
    "    return (grid_search.best_score_,)\n",
    "\n",
    "\n",
    "toolbox.register(\"evaluate\", evalFeatureSelection, X=X, y=y)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) 유전 연산 등록 (교차, 돌연변이, 선택)\n",
    "# ------------------------------------------------\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.02)  # mutation probability per gene\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) 메인 함수: GA 실행 + 최종 평가\n",
    "# ------------------------------------------------\n",
    "def main():\n",
    "    create_deap_classes()\n",
    "    random.seed(42)\n",
    "\n",
    "    # GA 초기 설정\n",
    "    population_size = 20  # 초기 인구 수\n",
    "    ngen = 10             # 세대 수\n",
    "    cxpb = 0.7            # 교차 확률\n",
    "    mutpb = 0.1           # 돌연변이 확률\n",
    "\n",
    "    # Early Stopping\n",
    "    patience = 3\n",
    "    best_fitness = -np.inf\n",
    "    no_improvement = 0\n",
    "\n",
    "    # 초기 인구 생성\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    best_fitness_per_gen = []\n",
    "    avg_fitness_per_gen = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # 1) 자손 생성 (교차 + 돌연변이)\n",
    "        offspring = algorithms.varAnd(population, toolbox, cxpb, mutpb)\n",
    "        \n",
    "        # 2) 각 자손 피트니스 평가 (GridSearchCV 수행)\n",
    "        fits = list(map(toolbox.evaluate, offspring))\n",
    "        for fit, ind in zip(fits, offspring):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # 3) 다음 세대 선택\n",
    "        population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "        # 4) 현재 세대의 Best\n",
    "        best_ind = tools.selBest(population, k=1)[0]\n",
    "        current_best_fitness = best_ind.fitness.values[0]\n",
    "\n",
    "        best_fitness_per_gen.append(current_best_fitness)\n",
    "        avg_fitness = np.mean([ind.fitness.values[0] for ind in population])\n",
    "        avg_fitness_per_gen.append(avg_fitness)\n",
    "        print(f\"[Generation {gen+1}] Best F1(macro) = {current_best_fitness:.4f}, Avg F1(macro) = {avg_fitness:.4f}\")\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        if current_best_fitness > best_fitness:\n",
    "            best_fitness = current_best_fitness\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"\\nEarly Stopping triggered at generation {gen+1}.\")\n",
    "            break\n",
    "\n",
    "    # (Optional) Fitness 진화 과정 시각화\n",
    "    if has_matplotlib:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(best_fitness_per_gen)+1), best_fitness_per_gen, \n",
    "                 label='Best F1(macro)')\n",
    "        plt.plot(range(1, len(avg_fitness_per_gen)+1), avg_fitness_per_gen, \n",
    "                 label='Average F1(macro)')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('F1 Score (macro)')\n",
    "        plt.title('Fitness Evolution with XGBoost + GridSearchCV')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 최종 선정된 특징 (best_ind)\n",
    "    best_ind = tools.selBest(population, k=1)[0]\n",
    "    selected_features = [i for i, bit in enumerate(best_ind) if bit == 1]\n",
    "    print(\"\\n--- 최종 선정된 특성 인덱스 ---\")\n",
    "    print(selected_features)\n",
    "    print(f\"Selected Feature Count: {len(selected_features)}\")\n",
    "\n",
    "    if len(selected_features) == 0:\n",
    "        print(\"선택된 특성이 없습니다. 성능 측정 불가.\")\n",
    "        return\n",
    "\n",
    "    # 최종 모델 학습/평가 (원하면 한 번 더 GridSearchCV 가능)\n",
    "    # 여기서는 간단히 \"best_ind\" 피처만 사용해 모델 성능 측정\n",
    "    final_xgb = XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # 5-Fold 교차검증\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X[:, selected_features], y), 1):\n",
    "        X_train, X_test = X[train_idx][:, selected_features], X[test_idx][:, selected_features]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        final_xgb.fit(X_train, y_train)\n",
    "        y_pred = final_xgb.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_m = f1_score(y_test, y_pred, average='weighted')\n",
    "        accuracy_list.append(acc)\n",
    "        f1_list.append(f1_m)\n",
    "        print(f\"Fold {fold_idx} => Accuracy: {acc:.4f}, F1: {f1_m:.4f}\")\n",
    "\n",
    "    final_accuracy = np.mean(accuracy_list)\n",
    "    final_f1 = np.mean(f1_list)\n",
    "    print(f\"\\n[최종 교차 검증 결과] Accuracy: {final_accuracy:.4f}, F1: {final_f1:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carly\\anaconda3\\envs\\dyspepsia_research\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "c:\\Users\\carly\\anaconda3\\envs\\dyspepsia_research\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer, accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "def create_deap_classes():\n",
    "    try:\n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "create_deap_classes()\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) 개체(Individual) 생성: 0/1 Feature Mask\n",
    "# ------------------------------------------------\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "\n",
    "def initialize_individual():\n",
    "    return tools.initRepeat(creator.Individual, toolbox.attr_bool, n=X.shape[1])\n",
    "\n",
    "toolbox.register(\"individual\", initialize_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) GA 평가함수: CatBoost + GridSearchCV (Weighted F1)\n",
    "# ------------------------------------------------\n",
    "def evalFeatureSelection(individual, X, y, seed, cv_folds=4):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return (0.0,)  # 아무 특성도 없으면 점수 0\n",
    "\n",
    "    param_grid = {\n",
    "        'iterations': [100, 200],\n",
    "        'depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'l2_leaf_reg': [1, 5],\n",
    "        'rsm': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    # CatBoost 모델\n",
    "    cat_model = CatBoostClassifier(\n",
    "        loss_function='MultiClass',\n",
    "        random_seed= seed,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # 교차검증/스코어링 설정\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    scoring = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=cat_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        cv=skf,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # 선택된 피처만으로 학습 + 튜닝\n",
    "    grid_search.fit(X[:, selected_features], y)\n",
    "\n",
    "    # grid_search.best_score_ => 교차검증 평균 Weighted F1\n",
    "    return (grid_search.best_score_,)\n",
    "\n",
    "\n",
    "\n",
    "def catboost(X, y, seed):\n",
    "    create_deap_classes()\n",
    "    \n",
    "    toolbox.register(\"evaluate\", evalFeatureSelection, X=X, y=y, seed=seed)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 5) 유전 연산 (교차, 돌연변이, 선택) 등록\n",
    "    # ------------------------------------------------\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.02)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # GA 파라미터\n",
    "    population_size = 20   # 초기 인구\n",
    "    ngen = 10              # 세대 수\n",
    "    cxpb = 0.7             # 교차 확률\n",
    "    mutpb = 0.1            # 돌연변이 확률\n",
    "\n",
    "    # Early Stopping\n",
    "    patience = 3\n",
    "    best_fitness = -np.inf\n",
    "    no_improvement = 0\n",
    "\n",
    "    # 1) 초기 인구 생성\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    best_fitness_per_gen = []\n",
    "    avg_fitness_per_gen = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # (a) 자손 생성 (교차+돌연변이)\n",
    "        offspring = algorithms.varAnd(population, toolbox, cxpb, mutpb)\n",
    "        \n",
    "        # (b) 각 자손의 피트니스 계산 (CatBoost + GridSearchCV)\n",
    "        fits = list(map(toolbox.evaluate, offspring))\n",
    "        for fit, ind in zip(fits, offspring):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # (c) 다음 세대 선택\n",
    "        population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "        # (d) 현재 세대 Best\n",
    "        best_ind = tools.selBest(population, k=1)[0]\n",
    "        current_best_fitness = best_ind.fitness.values[0]\n",
    "\n",
    "        best_fitness_per_gen.append(current_best_fitness)\n",
    "        avg_fitness = np.mean([ind.fitness.values[0] for ind in population])\n",
    "        avg_fitness_per_gen.append(avg_fitness)\n",
    "\n",
    "        print(f\"[Generation {gen+1}] Best F1(weighted) = {current_best_fitness:.4f}, Avg F1 = {avg_fitness:.4f}\")\n",
    "\n",
    "        # Early Stopping 여부 체크\n",
    "        if current_best_fitness > best_fitness:\n",
    "            best_fitness = current_best_fitness\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"\\nEarly Stopping triggered at generation {gen+1}.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    best_ind = tools.selBest(population, k=1)[0]\n",
    "    selected_features = [i for i, bit in enumerate(best_ind) if bit == 1]\n",
    "    print(\"\\n--- 최종 선정된 특성 인덱스 ---\")\n",
    "    print(selected_features)\n",
    "    print(f\"Selected Feature Count: {len(selected_features)}\")\n",
    "\n",
    "    if len(selected_features) == 0:\n",
    "        print(\"선택된 특성이 없습니다. 성능 측정 불가.\")\n",
    "        return\n",
    "\n",
    "    # 최종 평가(간단하게 CatBoost 기본 설정 + 5-Fold CV)\n",
    "    final_cat = CatBoostClassifier(loss_function='MultiClass', random_seed=42, verbose=0)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X[:, selected_features], y), 1):\n",
    "        X_train, X_test = X[train_idx][:, selected_features], X[test_idx][:, selected_features]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        final_cat.fit(X_train, y_train)\n",
    "        y_pred = final_cat.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_w = f1_score(y_test, y_pred, average='weighted')\n",
    "        accuracy_list.append(acc)\n",
    "        f1_list.append(f1_w)\n",
    "        print(f\"Fold {fold_idx} => Accuracy: {acc:.4f}, F1(weighted): {f1_w:.4f}\")\n",
    "\n",
    "    final_acc = np.mean(accuracy_list)\n",
    "    final_f1 = np.mean(f1_list)\n",
    "    print(f\"\\n[최종 교차 검증 결과] Accuracy: {final_acc:.4f}, F1(weighted): {final_f1:.4f}\")\n",
    "\n",
    "    return final_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (740854374.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 28\u001b[1;36m\u001b[0m\n\u001b[1;33m    result = result /\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset_link = [\n",
    "    \"../dataset/v2_seperate/mec.csv\",\n",
    "    \"../dataset/v2_seperate/skin.csv\",\n",
    "    \"../dataset/v2_seperate/survey.csv\",\n",
    "    \"../dataset/v2_seperate/v2_eda.csv\",\n",
    "    \"../dataset/v2_seperate/v2_original.csv\"\n",
    "]\n",
    "\n",
    "for link in dataset_link:\n",
    "    df = pd.read_csv(link)\n",
    "    y = df['diagnosis_type'].values #diagnosis : 6class, diagnosis_type : 3 class\n",
    "\n",
    "    df.drop(columns=['No', 'diagnosis', 'diagnosis_type'], inplace=True)\n",
    "\n",
    "    X = df.values\n",
    "    y = y - 1\n",
    "    \n",
    "    \n",
    "    ####Genetic-RandomForest######################################\n",
    "    ##################################################\n",
    "    ##################################################\n",
    "    seed_li = [1557, 2356, 88488]\n",
    "    result = 0\n",
    "    \n",
    "    for seed in seed_li:\n",
    "        result += catboost(X, y, seed)\n",
    "    \n",
    "    result = result / 3\n",
    "    print(link, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyspepsia_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
